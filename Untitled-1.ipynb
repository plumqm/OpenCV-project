{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "(2,)\n",
      "Iteration 1, loss = 0.62131675\n",
      "Iteration 2, loss = 0.62071361\n",
      "Iteration 3, loss = 0.61985404\n",
      "Iteration 4, loss = 0.61876443\n",
      "Iteration 5, loss = 0.61746862\n",
      "Iteration 6, loss = 0.61598821\n",
      "Iteration 7, loss = 0.61434271\n",
      "Iteration 8, loss = 0.61254975\n",
      "Iteration 9, loss = 0.61062524\n",
      "Iteration 10, loss = 0.60858354\n",
      "Iteration 11, loss = 0.60643760\n",
      "Iteration 12, loss = 0.60419908\n",
      "Iteration 13, loss = 0.60187851\n",
      "Iteration 14, loss = 0.59948534\n",
      "Iteration 15, loss = 0.59702809\n",
      "Iteration 16, loss = 0.59451443\n",
      "Iteration 17, loss = 0.59195124\n",
      "Iteration 18, loss = 0.58934471\n",
      "Iteration 19, loss = 0.58670040\n",
      "Iteration 20, loss = 0.58402330\n",
      "Iteration 21, loss = 0.58131789\n",
      "Iteration 22, loss = 0.57858817\n",
      "Iteration 23, loss = 0.57583775\n",
      "Iteration 24, loss = 0.57306985\n",
      "Iteration 25, loss = 0.57028735\n",
      "Iteration 26, loss = 0.56749283\n",
      "Iteration 27, loss = 0.56468861\n",
      "Iteration 28, loss = 0.56187673\n",
      "Iteration 29, loss = 0.55905904\n",
      "Iteration 30, loss = 0.55623718\n",
      "Iteration 31, loss = 0.55341130\n",
      "Iteration 32, loss = 0.55056363\n",
      "Iteration 33, loss = 0.54771111\n",
      "Iteration 34, loss = 0.54485507\n",
      "Iteration 35, loss = 0.54199670\n",
      "Iteration 36, loss = 0.53913707\n",
      "Iteration 37, loss = 0.53627712\n",
      "Iteration 38, loss = 0.53341771\n",
      "Iteration 39, loss = 0.53055958\n",
      "Iteration 40, loss = 0.52770341\n",
      "Iteration 41, loss = 0.52484979\n",
      "Iteration 42, loss = 0.52199928\n",
      "Iteration 43, loss = 0.51915232\n",
      "Iteration 44, loss = 0.51630937\n",
      "Iteration 45, loss = 0.51347078\n",
      "Iteration 46, loss = 0.51063690\n",
      "Iteration 47, loss = 0.50780803\n",
      "Iteration 48, loss = 0.50498444\n",
      "Iteration 49, loss = 0.50216637\n",
      "Iteration 50, loss = 0.49935403\n",
      "Iteration 51, loss = 0.49654761\n",
      "Iteration 52, loss = 0.49374729\n",
      "Iteration 53, loss = 0.49095323\n",
      "Iteration 54, loss = 0.48816556\n",
      "Iteration 55, loss = 0.48538441\n",
      "Iteration 56, loss = 0.48260990\n",
      "Iteration 57, loss = 0.47984213\n",
      "Iteration 58, loss = 0.47708121\n",
      "Iteration 59, loss = 0.47432721\n",
      "Iteration 60, loss = 0.47260072\n",
      "Iteration 61, loss = 0.46907833\n",
      "Iteration 62, loss = 0.46644834\n",
      "Iteration 63, loss = 0.46381609\n",
      "Iteration 64, loss = 0.46118254\n",
      "Iteration 65, loss = 0.45861068\n",
      "Iteration 66, loss = 0.45615419\n",
      "Iteration 67, loss = 0.45362513\n",
      "Iteration 68, loss = 0.45108833\n",
      "Iteration 69, loss = 0.44854524\n",
      "Iteration 70, loss = 0.44599721\n",
      "Iteration 71, loss = 0.44344545\n",
      "Iteration 72, loss = 0.44089106\n",
      "Iteration 73, loss = 0.43833507\n",
      "Iteration 74, loss = 0.43577839\n",
      "Iteration 75, loss = 0.43322188\n",
      "Iteration 76, loss = 0.43066629\n",
      "Iteration 77, loss = 0.42811235\n",
      "Iteration 78, loss = 0.42556070\n",
      "Iteration 79, loss = 0.42301192\n",
      "Iteration 80, loss = 0.42118594\n",
      "Iteration 81, loss = 0.41816197\n",
      "Iteration 82, loss = 0.41572679\n",
      "Iteration 83, loss = 0.41328672\n",
      "Iteration 84, loss = 0.41084304\n",
      "Iteration 85, loss = 0.40839692\n",
      "Iteration 86, loss = 0.40594944\n",
      "Iteration 87, loss = 0.40350158\n",
      "Iteration 88, loss = 0.40137565\n",
      "Iteration 89, loss = 0.39884265\n",
      "Iteration 90, loss = 0.39649947\n",
      "Iteration 91, loss = 0.39414939\n",
      "Iteration 92, loss = 0.39179393\n",
      "Iteration 93, loss = 0.38943448\n",
      "Iteration 94, loss = 0.38707228\n",
      "Iteration 95, loss = 0.38470849\n",
      "Iteration 96, loss = 0.38234418\n",
      "Iteration 97, loss = 0.37998029\n",
      "Iteration 98, loss = 0.37762710\n",
      "Iteration 99, loss = 0.37548683\n",
      "Iteration 100, loss = 0.37322763\n",
      "Iteration 101, loss = 0.37096236\n",
      "Iteration 102, loss = 0.36869252\n",
      "Iteration 103, loss = 0.36641948\n",
      "Iteration 104, loss = 0.36414449\n",
      "Iteration 105, loss = 0.36186870\n",
      "Iteration 106, loss = 0.35959315\n",
      "Iteration 107, loss = 0.35736239\n",
      "Iteration 108, loss = 0.35527006\n",
      "Iteration 109, loss = 0.35309628\n",
      "Iteration 110, loss = 0.35091662\n",
      "Iteration 111, loss = 0.34873264\n",
      "Iteration 112, loss = 0.34654572\n",
      "Iteration 113, loss = 0.34435716\n",
      "Iteration 114, loss = 0.34216812\n",
      "Iteration 115, loss = 0.33997967\n",
      "Iteration 116, loss = 0.33814923\n",
      "Iteration 117, loss = 0.33582446\n",
      "Iteration 118, loss = 0.33373554\n",
      "Iteration 119, loss = 0.33164136\n",
      "Iteration 120, loss = 0.32954344\n",
      "Iteration 121, loss = 0.32744319\n",
      "Iteration 122, loss = 0.32534188\n",
      "Iteration 123, loss = 0.32332199\n",
      "Iteration 124, loss = 0.32135066\n",
      "Iteration 125, loss = 0.31934230\n",
      "Iteration 126, loss = 0.31732787\n",
      "Iteration 127, loss = 0.31530900\n",
      "Iteration 128, loss = 0.31328719\n",
      "Iteration 129, loss = 0.31126378\n",
      "Iteration 130, loss = 0.30924001\n",
      "Iteration 131, loss = 0.30721700\n",
      "Iteration 132, loss = 0.30559439\n",
      "Iteration 133, loss = 0.30337823\n",
      "Iteration 134, loss = 0.30144878\n",
      "Iteration 135, loss = 0.29951489\n",
      "Iteration 136, loss = 0.29757808\n",
      "Iteration 137, loss = 0.29563971\n",
      "Iteration 138, loss = 0.29384931\n",
      "Iteration 139, loss = 0.29195813\n",
      "Iteration 140, loss = 0.29010507\n",
      "Iteration 141, loss = 0.28824630\n",
      "Iteration 142, loss = 0.28638344\n",
      "Iteration 143, loss = 0.28451800\n",
      "Iteration 144, loss = 0.28265133\n",
      "Iteration 145, loss = 0.28078468\n",
      "Iteration 146, loss = 0.27910100\n",
      "Iteration 147, loss = 0.27724190\n",
      "Iteration 148, loss = 0.27546058\n",
      "Iteration 149, loss = 0.27367494\n",
      "Iteration 150, loss = 0.27188650\n",
      "Iteration 151, loss = 0.27009665\n",
      "Iteration 152, loss = 0.26831672\n",
      "Iteration 153, loss = 0.26669713\n",
      "Iteration 154, loss = 0.26498626\n",
      "Iteration 155, loss = 0.26327038\n",
      "Iteration 156, loss = 0.26155106\n",
      "Iteration 157, loss = 0.25982977\n",
      "Iteration 158, loss = 0.25810780\n",
      "Iteration 159, loss = 0.25638635\n",
      "Iteration 160, loss = 0.25498192\n",
      "Iteration 161, loss = 0.25311935\n",
      "Iteration 162, loss = 0.25147754\n",
      "Iteration 163, loss = 0.24983234\n",
      "Iteration 164, loss = 0.24818520\n",
      "Iteration 165, loss = 0.24666701\n",
      "Iteration 166, loss = 0.24505472\n",
      "Iteration 167, loss = 0.24347872\n",
      "Iteration 168, loss = 0.24189790\n",
      "Iteration 169, loss = 0.24031383\n",
      "Iteration 170, loss = 0.23872791\n",
      "Iteration 171, loss = 0.23714144\n",
      "Iteration 172, loss = 0.23564258\n",
      "Iteration 173, loss = 0.23412787\n",
      "Iteration 174, loss = 0.23261240\n",
      "Iteration 175, loss = 0.23109331\n",
      "Iteration 176, loss = 0.22957204\n",
      "Iteration 177, loss = 0.22804989\n",
      "Iteration 178, loss = 0.22663490\n",
      "Iteration 179, loss = 0.22515722\n",
      "Iteration 180, loss = 0.22370226\n",
      "Iteration 181, loss = 0.22224369\n",
      "Iteration 182, loss = 0.22078294\n",
      "Iteration 183, loss = 0.21932130\n",
      "Iteration 184, loss = 0.21796968\n",
      "Iteration 185, loss = 0.21654278\n",
      "Iteration 186, loss = 0.21514528\n",
      "Iteration 187, loss = 0.21374439\n",
      "Iteration 188, loss = 0.21234147\n",
      "Iteration 189, loss = 0.21093780\n",
      "Iteration 190, loss = 0.20968247\n",
      "Iteration 191, loss = 0.20826886\n",
      "Iteration 192, loss = 0.20692670\n",
      "Iteration 193, loss = 0.20558143\n",
      "Iteration 194, loss = 0.20423440\n",
      "Iteration 195, loss = 0.20289539\n",
      "Iteration 196, loss = 0.20167073\n",
      "Iteration 197, loss = 0.20038038\n",
      "Iteration 198, loss = 0.19908641\n",
      "Iteration 199, loss = 0.19779019\n",
      "Iteration 200, loss = 0.19649295\n",
      "Iteration 201, loss = 0.19519581\n",
      "Iteration 202, loss = 0.19409571\n",
      "Iteration 203, loss = 0.19272923\n",
      "Iteration 204, loss = 0.19148997\n",
      "Iteration 205, loss = 0.19024856\n",
      "Iteration 206, loss = 0.18906054\n",
      "Iteration 207, loss = 0.18788346\n",
      "Iteration 208, loss = 0.18669263\n",
      "Iteration 209, loss = 0.18549825\n",
      "Iteration 210, loss = 0.18430164\n",
      "Iteration 211, loss = 0.18310397\n",
      "Iteration 212, loss = 0.18190633\n",
      "Iteration 213, loss = 0.18089121\n",
      "Iteration 214, loss = 0.17962734\n",
      "Iteration 215, loss = 0.17848242\n",
      "Iteration 216, loss = 0.17733560\n",
      "Iteration 217, loss = 0.17626366\n",
      "Iteration 218, loss = 0.17514937\n",
      "Iteration 219, loss = 0.17404887\n",
      "Iteration 220, loss = 0.17294528\n",
      "Iteration 221, loss = 0.17183982\n",
      "Iteration 222, loss = 0.17073358\n",
      "Iteration 223, loss = 0.16967497\n",
      "Iteration 224, loss = 0.16862584\n",
      "Iteration 225, loss = 0.16756617\n",
      "Iteration 226, loss = 0.16650431\n",
      "Iteration 227, loss = 0.16544136\n",
      "Iteration 228, loss = 0.16440866\n",
      "Iteration 229, loss = 0.16341480\n",
      "Iteration 230, loss = 0.16239562\n",
      "Iteration 231, loss = 0.16137413\n",
      "Iteration 232, loss = 0.16035144\n",
      "Iteration 233, loss = 0.15932851\n",
      "Iteration 234, loss = 0.15845321\n",
      "Iteration 235, loss = 0.15737895\n",
      "Iteration 236, loss = 0.15639951\n",
      "Iteration 237, loss = 0.15541847\n",
      "Iteration 238, loss = 0.15450684\n",
      "Iteration 239, loss = 0.15354579\n",
      "Iteration 240, loss = 0.15260341\n",
      "Iteration 241, loss = 0.15165860\n",
      "Iteration 242, loss = 0.15071240\n",
      "Iteration 243, loss = 0.14976575\n",
      "Iteration 244, loss = 0.14892137\n",
      "Iteration 245, loss = 0.14795999\n",
      "Iteration 246, loss = 0.14705263\n",
      "Iteration 247, loss = 0.14614372\n",
      "Iteration 248, loss = 0.14525432\n",
      "Iteration 249, loss = 0.14440743\n",
      "Iteration 250, loss = 0.14353370\n",
      "Iteration 251, loss = 0.14265773\n",
      "Iteration 252, loss = 0.14178049\n",
      "Iteration 253, loss = 0.14090287\n",
      "Iteration 254, loss = 0.14008256\n",
      "Iteration 255, loss = 0.13922774\n",
      "Iteration 256, loss = 0.13838615\n",
      "Iteration 257, loss = 0.13754319\n",
      "Iteration 258, loss = 0.13669972\n",
      "Iteration 259, loss = 0.13597584\n",
      "Iteration 260, loss = 0.13508900\n",
      "Iteration 261, loss = 0.13427966\n",
      "Iteration 262, loss = 0.13346893\n",
      "Iteration 263, loss = 0.13270722\n",
      "Iteration 264, loss = 0.13191872\n",
      "Iteration 265, loss = 0.13113878\n",
      "Iteration 266, loss = 0.13035693\n",
      "Iteration 267, loss = 0.12957404\n",
      "Iteration 268, loss = 0.12879088\n",
      "Iteration 269, loss = 0.12810806\n",
      "Iteration 270, loss = 0.12729477\n",
      "Iteration 271, loss = 0.12654334\n",
      "Iteration 272, loss = 0.12579080\n",
      "Iteration 273, loss = 0.12510570\n",
      "Iteration 274, loss = 0.12435129\n",
      "Iteration 275, loss = 0.12362733\n",
      "Iteration 276, loss = 0.12290177\n",
      "Iteration 277, loss = 0.12217539\n",
      "Iteration 278, loss = 0.12148607\n",
      "Iteration 279, loss = 0.12078593\n",
      "Iteration 280, loss = 0.12008745\n",
      "Iteration 281, loss = 0.11938761\n",
      "Iteration 282, loss = 0.11868713\n",
      "Iteration 283, loss = 0.11803095\n",
      "Iteration 284, loss = 0.11734711\n",
      "Iteration 285, loss = 0.11667370\n",
      "Iteration 286, loss = 0.11599910\n",
      "Iteration 287, loss = 0.11532400\n",
      "Iteration 288, loss = 0.11472354\n",
      "Iteration 289, loss = 0.11403232\n",
      "Iteration 290, loss = 0.11338336\n",
      "Iteration 291, loss = 0.11273333\n",
      "Iteration 292, loss = 0.11211374\n",
      "Iteration 293, loss = 0.11148825\n",
      "Iteration 294, loss = 0.11086204\n",
      "Iteration 295, loss = 0.11023443\n",
      "Iteration 296, loss = 0.10960608\n",
      "Iteration 297, loss = 0.10898155\n",
      "Iteration 298, loss = 0.10840273\n",
      "Iteration 299, loss = 0.10779788\n",
      "Iteration 300, loss = 0.10719188\n",
      "Iteration 301, loss = 0.10658537\n",
      "Iteration 302, loss = 0.10599889\n",
      "Iteration 303, loss = 0.10542384\n",
      "Iteration 304, loss = 0.10484025\n",
      "Iteration 305, loss = 0.10425569\n",
      "Iteration 306, loss = 0.10367074\n",
      "Iteration 307, loss = 0.10314653\n",
      "Iteration 308, loss = 0.10255042\n",
      "Iteration 309, loss = 0.10198769\n",
      "Iteration 310, loss = 0.10142412\n",
      "Iteration 311, loss = 0.10089821\n",
      "Iteration 312, loss = 0.10034356\n",
      "Iteration 313, loss = 0.09980026\n",
      "Iteration 314, loss = 0.09925582\n",
      "Iteration 315, loss = 0.09871081\n",
      "Iteration 316, loss = 0.09819839\n",
      "Iteration 317, loss = 0.09766607\n",
      "Iteration 318, loss = 0.09714109\n",
      "Iteration 319, loss = 0.09661519\n",
      "Iteration 320, loss = 0.09608890\n",
      "Iteration 321, loss = 0.09562468\n",
      "Iteration 322, loss = 0.09508006\n",
      "Iteration 323, loss = 0.09457332\n",
      "Iteration 324, loss = 0.09406580\n",
      "Iteration 325, loss = 0.09360042\n",
      "Iteration 326, loss = 0.09309196\n",
      "Iteration 327, loss = 0.09260235\n",
      "Iteration 328, loss = 0.09211171\n",
      "Iteration 329, loss = 0.09162057\n",
      "Iteration 330, loss = 0.09117093\n",
      "Iteration 331, loss = 0.09067838\n",
      "Iteration 332, loss = 0.09020497\n",
      "Iteration 333, loss = 0.08973075\n",
      "Iteration 334, loss = 0.08926849\n",
      "Iteration 335, loss = 0.08882022\n",
      "Iteration 336, loss = 0.08836235\n",
      "Iteration 337, loss = 0.08790347\n",
      "Iteration 338, loss = 0.08744407\n",
      "Iteration 339, loss = 0.08699009\n",
      "Iteration 340, loss = 0.08656229\n",
      "Iteration 341, loss = 0.08611920\n",
      "Iteration 342, loss = 0.08567532\n",
      "Iteration 343, loss = 0.08523108\n",
      "Iteration 344, loss = 0.08482049\n",
      "Iteration 345, loss = 0.08437850\n",
      "Iteration 346, loss = 0.08395027\n",
      "Iteration 347, loss = 0.08352139\n",
      "Iteration 348, loss = 0.08311587\n",
      "Iteration 349, loss = 0.08269748\n",
      "Iteration 350, loss = 0.08228330\n",
      "Iteration 351, loss = 0.08186827\n",
      "Iteration 352, loss = 0.08145281\n",
      "Iteration 353, loss = 0.08106886\n",
      "Iteration 354, loss = 0.08065492\n",
      "Iteration 355, loss = 0.08025406\n",
      "Iteration 356, loss = 0.07985253\n",
      "Iteration 357, loss = 0.07946553\n",
      "Iteration 358, loss = 0.07908072\n",
      "Iteration 359, loss = 0.07869267\n",
      "Iteration 360, loss = 0.07830378\n",
      "Iteration 361, loss = 0.07791446\n",
      "Iteration 362, loss = 0.07754290\n",
      "Iteration 363, loss = 0.07716638\n",
      "Iteration 364, loss = 0.07679051\n",
      "Iteration 365, loss = 0.07641399\n",
      "Iteration 366, loss = 0.07603852\n",
      "Iteration 367, loss = 0.07568992\n",
      "Iteration 368, loss = 0.07532585\n",
      "Iteration 369, loss = 0.07496098\n",
      "Iteration 370, loss = 0.07459570\n",
      "Iteration 371, loss = 0.07423490\n",
      "Iteration 372, loss = 0.07389348\n",
      "Iteration 373, loss = 0.07354066\n",
      "Iteration 374, loss = 0.07318721\n",
      "Iteration 375, loss = 0.07283347\n",
      "Iteration 376, loss = 0.07251438\n",
      "Iteration 377, loss = 0.07215354\n",
      "Iteration 378, loss = 0.07181206\n",
      "Iteration 379, loss = 0.07147006\n",
      "Iteration 380, loss = 0.07116166\n",
      "Iteration 381, loss = 0.07081210\n",
      "Iteration 382, loss = 0.07048138\n",
      "Iteration 383, loss = 0.07014998\n",
      "Iteration 384, loss = 0.06982705\n",
      "Iteration 385, loss = 0.06951201\n",
      "Iteration 386, loss = 0.06919115\n",
      "Iteration 387, loss = 0.06886953\n",
      "Iteration 388, loss = 0.06854747\n",
      "Iteration 389, loss = 0.06823009\n",
      "Iteration 390, loss = 0.06792777\n",
      "Iteration 391, loss = 0.06761635\n",
      "Iteration 392, loss = 0.06730434\n",
      "Iteration 393, loss = 0.06699204\n",
      "Iteration 394, loss = 0.06670804\n",
      "Iteration 395, loss = 0.06639121\n",
      "Iteration 396, loss = 0.06608943\n",
      "Iteration 397, loss = 0.06578716\n",
      "Iteration 398, loss = 0.06551214\n",
      "Iteration 399, loss = 0.06520514\n",
      "Iteration 400, loss = 0.06491257\n",
      "Iteration 401, loss = 0.06461939\n",
      "Iteration 402, loss = 0.06433246\n",
      "Iteration 403, loss = 0.06405453\n",
      "Iteration 404, loss = 0.06377043\n",
      "Iteration 405, loss = 0.06348564\n",
      "Iteration 406, loss = 0.06320045\n",
      "Iteration 407, loss = 0.06291982\n",
      "Iteration 408, loss = 0.06265126\n",
      "Iteration 409, loss = 0.06237526\n",
      "Iteration 410, loss = 0.06209872\n",
      "Iteration 411, loss = 0.06182190\n",
      "Iteration 412, loss = 0.06157207\n",
      "Iteration 413, loss = 0.06128895\n",
      "Iteration 414, loss = 0.06102124\n",
      "Iteration 415, loss = 0.06075436\n",
      "Iteration 416, loss = 0.06050490\n",
      "Iteration 417, loss = 0.06024502\n",
      "Iteration 418, loss = 0.05998440\n",
      "Iteration 419, loss = 0.05972332\n",
      "Iteration 420, loss = 0.05946200\n",
      "Iteration 421, loss = 0.05921650\n",
      "Iteration 422, loss = 0.05895878\n",
      "Iteration 423, loss = 0.05870606\n",
      "Iteration 424, loss = 0.05845293\n",
      "Iteration 425, loss = 0.05822142\n",
      "Iteration 426, loss = 0.05796505\n",
      "Iteration 427, loss = 0.05771982\n",
      "Iteration 428, loss = 0.05747408\n",
      "Iteration 429, loss = 0.05723789\n",
      "Iteration 430, loss = 0.05700015\n",
      "Iteration 431, loss = 0.05676179\n",
      "Iteration 432, loss = 0.05652286\n",
      "Iteration 433, loss = 0.05628357\n",
      "Iteration 434, loss = 0.05605712\n",
      "Iteration 435, loss = 0.05582231\n",
      "Iteration 436, loss = 0.05559049\n",
      "Iteration 437, loss = 0.05535822\n",
      "Iteration 438, loss = 0.05513249\n",
      "Iteration 439, loss = 0.05491016\n",
      "Iteration 440, loss = 0.05468484\n",
      "Iteration 441, loss = 0.05445898\n",
      "Iteration 442, loss = 0.05423281\n",
      "Iteration 443, loss = 0.05402031\n",
      "Iteration 444, loss = 0.05379669\n",
      "Iteration 445, loss = 0.05357753\n",
      "Iteration 446, loss = 0.05335794\n",
      "Iteration 447, loss = 0.05314823\n",
      "Iteration 448, loss = 0.05293422\n",
      "Iteration 449, loss = 0.05272114\n",
      "Iteration 450, loss = 0.05250757\n",
      "Iteration 451, loss = 0.05229368\n",
      "Iteration 452, loss = 0.05209810\n",
      "Iteration 453, loss = 0.05188115\n",
      "Iteration 454, loss = 0.05167383\n",
      "Iteration 455, loss = 0.05146611\n",
      "Iteration 456, loss = 0.05127405\n",
      "Iteration 457, loss = 0.05106517\n",
      "Iteration 458, loss = 0.05086355\n",
      "Iteration 459, loss = 0.05066145\n",
      "Iteration 460, loss = 0.05046124\n",
      "Iteration 461, loss = 0.05027116\n",
      "Iteration 462, loss = 0.05007481\n",
      "Iteration 463, loss = 0.04987794\n",
      "Iteration 464, loss = 0.04968074\n",
      "Iteration 465, loss = 0.04948563\n",
      "Iteration 466, loss = 0.04930011\n",
      "Iteration 467, loss = 0.04910878\n",
      "Iteration 468, loss = 0.04891702\n",
      "Iteration 469, loss = 0.04872502\n",
      "Iteration 470, loss = 0.04855194\n",
      "Iteration 471, loss = 0.04835452\n",
      "Iteration 472, loss = 0.04816837\n",
      "Iteration 473, loss = 0.04798538\n",
      "Iteration 474, loss = 0.04780856\n",
      "Iteration 475, loss = 0.04762744\n",
      "Iteration 476, loss = 0.04744577\n",
      "Iteration 477, loss = 0.04726373\n",
      "Iteration 478, loss = 0.04708147\n",
      "Iteration 479, loss = 0.04691529\n",
      "Iteration 480, loss = 0.04672975\n",
      "Iteration 481, loss = 0.04655307\n",
      "Iteration 482, loss = 0.04638038\n",
      "Iteration 483, loss = 0.04621149\n",
      "Iteration 484, loss = 0.04603956\n",
      "Iteration 485, loss = 0.04586713\n",
      "Iteration 486, loss = 0.04569435\n",
      "Iteration 487, loss = 0.04552138\n",
      "Iteration 488, loss = 0.04536807\n",
      "Iteration 489, loss = 0.04518749\n",
      "Iteration 490, loss = 0.04501977\n",
      "Iteration 491, loss = 0.04486140\n",
      "Iteration 492, loss = 0.04469544\n",
      "Iteration 493, loss = 0.04453220\n",
      "Iteration 494, loss = 0.04436849\n",
      "Iteration 495, loss = 0.04420444\n",
      "Iteration 496, loss = 0.04404546\n",
      "Iteration 497, loss = 0.04388744\n",
      "Iteration 498, loss = 0.04372804\n",
      "Iteration 499, loss = 0.04356825\n",
      "Iteration 500, loss = 0.04340821\n",
      "Iteration 501, loss = 0.04326467\n",
      "Iteration 502, loss = 0.04309905\n",
      "Iteration 503, loss = 0.04294368\n",
      "Iteration 504, loss = 0.04279122\n",
      "Iteration 505, loss = 0.04264303\n",
      "Iteration 506, loss = 0.04249165\n",
      "Iteration 507, loss = 0.04233980\n",
      "Iteration 508, loss = 0.04218761\n",
      "Iteration 509, loss = 0.04203522\n",
      "Iteration 510, loss = 0.04189574\n",
      "Iteration 511, loss = 0.04174081\n",
      "Iteration 512, loss = 0.04159288\n",
      "Iteration 513, loss = 0.04144797\n",
      "Iteration 514, loss = 0.04130661\n",
      "Iteration 515, loss = 0.04116250\n",
      "Iteration 516, loss = 0.04101795\n",
      "Iteration 517, loss = 0.04087308\n",
      "Iteration 518, loss = 0.04072803\n",
      "Iteration 519, loss = 0.04059909\n",
      "Iteration 520, loss = 0.04044775\n",
      "Iteration 521, loss = 0.04030693\n",
      "Iteration 522, loss = 0.04017396\n",
      "Iteration 523, loss = 0.04003437\n",
      "Iteration 524, loss = 0.03989717\n",
      "Iteration 525, loss = 0.03975954\n",
      "Iteration 526, loss = 0.03962162\n",
      "Iteration 527, loss = 0.03948840\n",
      "Iteration 528, loss = 0.03935485\n",
      "Iteration 529, loss = 0.03922068\n",
      "Iteration 530, loss = 0.03908617\n",
      "Iteration 531, loss = 0.03895143\n",
      "Iteration 532, loss = 0.03883121\n",
      "Iteration 533, loss = 0.03869090\n",
      "Iteration 534, loss = 0.03855994\n",
      "Iteration 535, loss = 0.03843252\n",
      "Iteration 536, loss = 0.03830630\n",
      "Iteration 537, loss = 0.03817858\n",
      "Iteration 538, loss = 0.03805044\n",
      "Iteration 539, loss = 0.03792200\n",
      "Iteration 540, loss = 0.03779336\n",
      "Iteration 541, loss = 0.03767698\n",
      "Iteration 542, loss = 0.03754463\n",
      "Iteration 543, loss = 0.03741962\n",
      "Iteration 544, loss = 0.03729887\n",
      "Iteration 545, loss = 0.03717750\n",
      "Iteration 546, loss = 0.03705560\n",
      "Iteration 547, loss = 0.03693332\n",
      "Iteration 548, loss = 0.03681075\n",
      "Iteration 549, loss = 0.03668896\n",
      "Iteration 550, loss = 0.03657351\n",
      "Iteration 551, loss = 0.03645418\n",
      "Iteration 552, loss = 0.03633452\n",
      "Iteration 553, loss = 0.03621465\n",
      "Iteration 554, loss = 0.03610388\n",
      "Iteration 555, loss = 0.03598271\n",
      "Iteration 556, loss = 0.03586611\n",
      "Iteration 557, loss = 0.03574923\n",
      "Iteration 558, loss = 0.03564244\n",
      "Iteration 559, loss = 0.03552293\n",
      "Iteration 560, loss = 0.03540909\n",
      "Iteration 561, loss = 0.03529493\n",
      "Iteration 562, loss = 0.03518552\n",
      "Iteration 563, loss = 0.03507380\n",
      "Iteration 564, loss = 0.03496250\n",
      "Iteration 565, loss = 0.03485087\n",
      "Iteration 566, loss = 0.03473900\n",
      "Iteration 567, loss = 0.03463388\n",
      "Iteration 568, loss = 0.03452242\n",
      "Iteration 569, loss = 0.03441350\n",
      "Iteration 570, loss = 0.03430429\n",
      "Iteration 571, loss = 0.03420001\n",
      "Iteration 572, loss = 0.03409273\n",
      "Iteration 573, loss = 0.03398628\n",
      "Iteration 574, loss = 0.03387951\n",
      "Iteration 575, loss = 0.03377253\n",
      "Iteration 576, loss = 0.03367485\n",
      "Iteration 577, loss = 0.03356537\n",
      "Iteration 578, loss = 0.03346120\n",
      "Iteration 579, loss = 0.03335676\n",
      "Iteration 580, loss = 0.03326110\n",
      "Iteration 581, loss = 0.03315441\n",
      "Iteration 582, loss = 0.03305259\n",
      "Iteration 583, loss = 0.03295048\n",
      "Iteration 584, loss = 0.03285155\n",
      "Iteration 585, loss = 0.03275255\n",
      "Iteration 586, loss = 0.03265292\n",
      "Iteration 587, loss = 0.03255298\n",
      "Iteration 588, loss = 0.03245281\n",
      "Iteration 589, loss = 0.03235695\n",
      "Iteration 590, loss = 0.03225875\n",
      "Iteration 591, loss = 0.03216114\n",
      "Iteration 592, loss = 0.03206327\n",
      "Iteration 593, loss = 0.03196784\n",
      "Iteration 594, loss = 0.03187356\n",
      "Iteration 595, loss = 0.03177809\n",
      "Iteration 596, loss = 0.03168232\n",
      "Iteration 597, loss = 0.03158635\n",
      "Iteration 598, loss = 0.03149655\n",
      "Iteration 599, loss = 0.03140042\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "# 分类器\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# 回归\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = np.array([[0.,0.],\n",
    "             [1.,1.]])\n",
    "\n",
    "Y = np.array([0,1])\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "'''\n",
    "solver选择优化算法 ,sgd 随机梯度下降算法\n",
    "alpha 正则项系数\n",
    "activation 激活函数，隐藏层（hidden layer）中使用\n",
    "hidden_layer_sizes = (5, 2),第一个隐藏层有5个节点，第二个隐藏层有2个节点\n",
    "max_iter 最大迭代次数\n",
    "tol->tolerance 对于连续10次loss差值\n",
    "verbose是否输出中途信息\n",
    "'''\n",
    "clf = MLPClassifier(solver='sgd', alpha=1e-5, activation='relu',\n",
    "                    hidden_layer_sizes=(5, 2), max_iter=2000, tol=1e-4, verbose=True)\n",
    "clf.fit(X,Y)\n",
    "\n",
    "preficted_value = clf.predict([[2,2],\n",
    "                               [-1,-2]])\n",
    "\n",
    "print(preficted_value)\n",
    "print([coef.shape for coef in clf.coefs_])              \n",
    "print([coef for coef in clf.coefs_])              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mVariable(\u001b[38;5;241m3\u001b[39m,name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mVariable(\u001b[38;5;241m4\u001b[39m,name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
